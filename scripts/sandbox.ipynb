{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "from imwatermark import WatermarkEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.models.diffusion.dpm_solver import DPMSolverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESRGAN Imports\n",
    "import os.path as osp\n",
    "import glob\n",
    "import RRDBNet_arch as arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, device=torch.device(\"cuda\"), verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        model.cuda()\n",
    "    elif device == torch.device(\"cpu\"):\n",
    "        model.cpu()\n",
    "        model.cond_stage_model.device = \"cpu\"\n",
    "    else:\n",
    "        raise ValueError(f\"Incorrect device name. Received: {device}\")\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ESRGAN model\n",
    "esrgan_model_path = './esrgan_models/RRDB_ESRGAN_x4.pth'\n",
    "esrgan_device = torch.device('cuda')\n",
    "esrgan_model = arch.RRDBNet(3, 3, 64, 23, gc=32)\n",
    "esrgan_model.load_state_dict(torch.load(esrgan_model_path), strict=True)\n",
    "esrgan_model.eval()\n",
    "esrgan_model = esrgan_model.to(esrgan_device)\n",
    "\n",
    "def upscale_samples(samples_img_gen):\n",
    "    samples_img_gen = torch.cat(samples_img_gen, dim=0).to(torch.float32)\n",
    "    with torch.no_grad():\n",
    "        output = esrgan_model(samples_img_gen).data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "    if len(output.shape) == 3:\n",
    "        output = np.array([output])\n",
    "    output = np.transpose(output, (0, 2, 3, 1))\n",
    "    output = (output * 255.0).round()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../model_weights/v2-1_512-ema-pruned.ckpt\n",
      "Global Step: 220000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n"
     ]
    }
   ],
   "source": [
    "# Set up stable diffusion model\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "config = OmegaConf.load(\"../configs/stable-diffusion/v2-inference.yaml\")\n",
    "device = torch.device(\"cuda\")\n",
    "model = load_model_from_config(config, \"../model_weights/v2-1_512-ema-pruned.ckpt\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "plms = False\n",
    "dpm = False\n",
    "if plms:\n",
    "    sampler = PLMSSampler(model, device=device)\n",
    "elif dpm:\n",
    "    sampler = DPMSolverSampler(model, device=device)\n",
    "else:\n",
    "    sampler = DDIMSampler(model, device=device)\n",
    "outpath = '../outputs/sandbox'\n",
    "\n",
    "n_samples=1\n",
    "batch_size = 1\n",
    "n_rows = 1\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "sample_count = 0\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(outpath)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2086993\n",
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 72, 128), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "DDIM Sampler: 100%|██████████| 50/50 [00:05<00:00,  9.17it/s]\n",
      "data: 100%|██████████| 1/1 [00:05<00:00,  5.70s/it]\n",
      "Sampling: 100%|██████████| 1/1 [00:05<00:00,  5.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upscaling...\n",
      "Done. 9.691\n"
     ]
    }
   ],
   "source": [
    "start_code = None\n",
    "precision_scope = autocast\n",
    "sampler = DDIMSampler(model, device=device)\n",
    "scale = 9\n",
    "opt_C = 4\n",
    "opt_H = 576\n",
    "opt_f = 8\n",
    "opt_W = 1024\n",
    "steps=50\n",
    "ddim_eta = 0.0\n",
    "seed_everything(np.random.randint(9999999))\n",
    "\n",
    "prompt = 'A realistic, highly-detailed australian shepherd catching a fish with its mouth from a river in the countryside, mountainous background, beautiful sunny day, symmetrical face, beautiful eyes, detailed eyes, detailed paws, symmetrical legs, realistic fur, high-resolution.'\n",
    "data = [batch_size * [prompt]]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad(), \\\n",
    "    precision_scope(\"cuda\"), \\\n",
    "    model.ema_scope():\n",
    "        all_samples = list()\n",
    "        for n in trange(n_samples, desc=\"Sampling\"):\n",
    "            for prompts in tqdm(data, desc=\"data\"):\n",
    "                uc = None\n",
    "                if scale != 1.0:\n",
    "                    uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                if isinstance(prompts, tuple):\n",
    "                    prompts = list(prompts)\n",
    "                c = model.get_learned_conditioning(prompts)\n",
    "                shape = [opt_C, opt_H // opt_f, opt_W // opt_f]\n",
    "                samples, _ = sampler.sample(S=steps,\n",
    "                                                    conditioning=c,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shape=shape,\n",
    "                                                    verbose=False,\n",
    "                                                    unconditional_guidance_scale=scale,\n",
    "                                                    unconditional_conditioning=uc,\n",
    "                                                    eta=ddim_eta,\n",
    "                                                    x_T=start_code)\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples)\n",
    "                x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                all_samples.append(x_samples)\n",
    "\n",
    "print('Upscaling...')\n",
    "upscaled_samples = upscale_samples(all_samples)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Done. {round(end_time-start_time, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for upscaled_img in upscaled_samples:\n",
    "    img = Image.fromarray(upscaled_img.astype(np.uint8))\n",
    "    img.save(os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "    base_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x_samples in all_samples:\n",
    "#     for x_sample in x_samples:\n",
    "#         x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "#         img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "#         img.save(os.path.join(sample_path, f\"{base_count:05}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import time\n",
    "import concurrent.futures\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import sys\n",
    "import threading\n",
    "import torch\n",
    "import gc\n",
    "from PyQt5.QtWidgets import QApplication, QLabel, QWidget, QVBoxLayout\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "from PyQt5.QtCore import pyqtSignal, pyqtSlot, Qt, QThread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../model_weights/v2-1_512-ema-pruned.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 220000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n"
     ]
    }
   ],
   "source": [
    "esrgan_model = initialize_esrgan()\n",
    "model = init_sd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = '''A realistic, highly-detailed australian shepherd catching a fish with \\\n",
    "    its mouth from a river in the countryside, mountainous background, beautiful \\\n",
    "    sunny day, symmetrical face, beautiful eyes, detailed eyes, detailed paws, \\\n",
    "    symmetrical legs, realistic fur, high-resolution.'''\n",
    "prompt1 = '''A suureal, stylistic australian shepherd catching an ethereal glowing fish with \\\n",
    "    its mouth from a majestic psychedelic river in the colorful countryside, mountainous background, beautiful \\\n",
    "    sunny day, symmetrical face, beautiful eyes, detailed eyes, detailed paws, \\\n",
    "    symmetrical legs, Salvador Dali, surreal.'''\n",
    "prompt2 = '''A tiger stalking its prey in the woods at dusk, dark, eerie, creepy, beautiful and majestic, ethereal, \\\n",
    "    surreal and psychedelic, in the style of Bosch, during dusk, big moon, detailed paws.'''\n",
    "options = get_options_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2328217\n",
      "data: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:00<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 72, 128), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:03<00:00, 16.41it/s]\n",
      "Global seed set to 2328217\n",
      "data: 100%|██████████| 1/1 [00:00<00:00,  6.12it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:00<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 72, 128), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:02<00:00, 18.53it/s]\n"
     ]
    }
   ],
   "source": [
    "ucs, cs, text_enc_time = encode_text(model, prompt1, options)\n",
    "encoded_samples1, sample_enc_time = encode_samples(model, ucs, cs, options)\n",
    "\n",
    "ucs, cs, text_enc_time = encode_text(model, prompt2, options)\n",
    "encoded_samples2, sample_enc_time = encode_samples(model, ucs, cs, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream_step_size = 35\n",
    "diff_vec = (encoded_samples2[0] - encoded_samples1[0]) / dream_step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDisplay(QWidget):\n",
    "    update_image = pyqtSignal(QImage)\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image_label = QLabel(self)\n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.image_label)\n",
    "        self.setLayout(layout)\n",
    "        self.update_image.connect(self.setImage)\n",
    "\n",
    "    @pyqtSlot(QImage)\n",
    "    def setImage(self, image):\n",
    "        pixmap = QPixmap.fromImage(image)\n",
    "        self.image_label.setPixmap(pixmap)\n",
    "\n",
    "\n",
    "def upscale_image_nn(array, new_size):\n",
    "    return cv2.resize(array, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def bilinear_interpolation(array, new_size):\n",
    "    return cv2.resize(array, new_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def bicubic_interpolation(array, new_size):\n",
    "    return cv2.resize(array, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def esrgan_upscale(raw_imgs):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        images = esrgan_model(raw_imgs.to(torch.float32))\n",
    "    end_time = time.time()\n",
    "    return images, (end_time - start_time)\n",
    "\n",
    "\n",
    "def convert_array_to_qimage(array):\n",
    "    array = upscale_image_nn(array, (3840, 2160))\n",
    "    height, width, channels = array.shape\n",
    "    bytes_per_line = channels * width\n",
    "    array = array.astype(np.uint8)\n",
    "    qimage = QImage(array.tobytes(), width, height, bytes_per_line, QImage.Format_RGB888)\n",
    "    return qimage\n",
    "\n",
    "\n",
    "def generate_next_frames(model, encoded_samples):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad(), \\\n",
    "        options['precision_scope'](\"cuda\"), \\\n",
    "        model.ema_scope():\n",
    "            x_samples = model.decode_first_stage(encoded_samples)\n",
    "            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "    end_time = time.time()\n",
    "    return x_samples, (end_time - start_time)\n",
    "\n",
    "\n",
    "def dream_thread(display_widget):\n",
    "    dream_times = list()\n",
    "    frame_times = list()\n",
    "    cleaning_times = list()\n",
    "    display_times = list()\n",
    "    encoded_dream_all_samples = [encoded_samples1[0] + diff_vec * i for i in range(75)]\n",
    "    encoded_dream_all_samples = torch.cat(encoded_dream_all_samples, dim=0)\n",
    "    batch_size = 1\n",
    "    clean_freq = 2\n",
    "\n",
    "    while True:\n",
    "        for i in range(0, dream_step_size, batch_size):\n",
    "            start_time = time.time()\n",
    "            dream_samples, dream_gen_time = generate_next_frames(model, encoded_dream_all_samples[i:i+1])\n",
    "            dream_samples, upscale_time = esrgan_upscale(dream_samples)\n",
    "            dream_samples = (dream_samples * 255).to(torch.uint8)\n",
    "            dream_samples = dream_samples.permute(0, 2, 3, 1).to('cpu', non_blocking=True).numpy()\n",
    "\n",
    "            start_display = time.time()\n",
    "            for dream in dream_samples:\n",
    "                qimage = convert_array_to_qimage(dream)\n",
    "                display_widget.update_image.emit(qimage)\n",
    "            end_display = time.time()\n",
    "\n",
    "            if (i / batch_size) % clean_freq == 0:\n",
    "                start_cleaning = time.time()\n",
    "                # torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                end_cleaning = time.time()\n",
    "\n",
    "            end_time = time.time()\n",
    "            dream_times.append(dream_gen_time)\n",
    "            display_times.append(end_display - start_display)\n",
    "            cleaning_times.append(end_cleaning - start_cleaning)\n",
    "            frame_times.append(end_time - start_time)\n",
    "        for i in range(dream_step_size-1, 0, -batch_size):\n",
    "            start_time = time.time()\n",
    "            dream_samples, dream_gen_time = generate_next_frames(model, encoded_dream_all_samples[i:i+1])\n",
    "            dream_samples = (dream_samples * 255).to(torch.uint8)\n",
    "            dream_samples = dream_samples.permute(0, 2, 3, 1).to('cpu', non_blocking=False).numpy()\n",
    "\n",
    "            start_display = time.time()\n",
    "            for dream in dream_samples:\n",
    "                qimage = convert_array_to_qimage(dream)\n",
    "                display_widget.update_image.emit(qimage)\n",
    "            end_display = time.time()\n",
    "\n",
    "            if (i / batch_size) % clean_freq == 0:\n",
    "                start_cleaning = time.time()\n",
    "                # torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                end_cleaning = time.time()\n",
    "\n",
    "            end_time = time.time()\n",
    "            dream_times.append(dream_gen_time)\n",
    "            display_times.append(end_display - start_display)\n",
    "            cleaning_times.append(end_cleaning - start_cleaning)\n",
    "            frame_times.append(end_time - start_time)\n",
    "        print(f'Average dream time: {sum(dream_times) / len(dream_times)}')\n",
    "        print(f'Average display time: {sum(display_times) / len(display_times)}')\n",
    "        print(f'Average cleaning time: {sum(cleaning_times) / len(cleaning_times)}')\n",
    "        print(f'Average time per frame: {sum(frame_times) / len(frame_times)}')\n",
    "\n",
    "\n",
    "def start_gui_and_processing():\n",
    "    app = QApplication.instance()\n",
    "    if not app:\n",
    "        app = QApplication(sys.argv)\n",
    "\n",
    "    display_widget = ImageDisplay()\n",
    "    display_widget.show()\n",
    "\n",
    "    thread = threading.Thread(target=dream_thread, args=(display_widget,))\n",
    "    thread.start()\n",
    "\n",
    "    app.exec_()\n",
    "\n",
    "    display_widget.deleteLater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dream_all_samples = [encoded_samples1[0] + diff_vec * i for i in range(75)]\n",
    "encoded_dream_all_samples = torch.cat(encoded_dream_all_samples, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-6.8242, device='cuda:0')\n",
      "tensor(7.7960, device='cuda:0')\n",
      "tensor(0.0812, device='cuda:0')\n",
      "torch.Size([4, 72, 128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "smpl = 74\n",
    "print(encoded_dream_all_samples[smpl].min())\n",
    "print(encoded_dream_all_samples[smpl].max())\n",
    "print(encoded_dream_all_samples[smpl].mean())\n",
    "print(encoded_dream_all_samples[smpl].shape)\n",
    "print(encoded_dream_all_samples[smpl].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "start_gui_and_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4, 72, 128)\n",
    "std_dev = np.random.randint(1, 4)\n",
    "random_gaussian_tensor = torch.randn(shape, dtype=torch.float32) * std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.8649)\n",
      "tensor(8.3408)\n",
      "tensor(-0.0080)\n",
      "torch.Size([4, 72, 128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(random_gaussian_tensor.min())\n",
    "print(random_gaussian_tensor.max())\n",
    "print(random_gaussian_tensor.mean())\n",
    "print(random_gaussian_tensor.shape)\n",
    "print(random_gaussian_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

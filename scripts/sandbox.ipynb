{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "from imwatermark import WatermarkEncoder\n",
    "import time\n",
    "\n",
    "sys.path.append('c:\\\\Users\\\\Owner\\\\Documents\\\\image_generation\\\\rapid_diffusion_dreamer')\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.models.diffusion.dpm_solver import DPMSolverSampler\n",
    "\n",
    "# ESRGAN Imports\n",
    "import os.path as osp\n",
    "import glob\n",
    "import RRDBNet_arch as arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, device=torch.device(\"cuda\"), verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        model.cuda()\n",
    "    elif device == torch.device(\"cpu\"):\n",
    "        model.cpu()\n",
    "        model.cond_stage_model.device = \"cpu\"\n",
    "    else:\n",
    "        raise ValueError(f\"Incorrect device name. Received: {device}\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_sd():\n",
    "    config = OmegaConf.load(\"../configs/stable-diffusion/v2-inference.yaml\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    return load_model_from_config(config, \"../model_weights/v2-1_512-ema-pruned.ckpt\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_esrgan(esrgan_model_path = './esrgan_models/RRDB_ESRGAN_x4.pth'):\n",
    "    esrgan_device = torch.device('cuda')\n",
    "    esrgan_model = arch.RRDBNet(3, 3, 64, 23, gc=32)\n",
    "    esrgan_model.load_state_dict(torch.load(esrgan_model_path), strict=True)\n",
    "    esrgan_model.eval()\n",
    "    return esrgan_model.to(esrgan_device)\n",
    "\n",
    "\n",
    "def upscale_images(raw_imgs, esrgan_model):\n",
    "    start_time = time.time()\n",
    "    raw_imgs = torch.cat(raw_imgs, dim=0).to(torch.float32)\n",
    "    with torch.no_grad():\n",
    "        images = esrgan_model(raw_imgs).data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "    if len(images.shape) == 3: # Single image (add batch dimension for consistency)\n",
    "        images = np.array([images])\n",
    "    images = np.transpose(images, (0, 2, 3, 1))\n",
    "    images = (images * 255.0).round()\n",
    "    end_time = time.time()\n",
    "    return images, (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_options_dict(model, steps = 50, h = 576, w = 1024, rand_seed = True, outpath = 'sandbox', plms = False, dpm = False):\n",
    "    options = {}\n",
    "    options['device'] = torch.device(\"cuda\")\n",
    "    if plms:\n",
    "        options['sampler'] = PLMSSampler(model, device=options['device'])\n",
    "    elif dpm:\n",
    "        options['sampler'] = DPMSolverSampler(model, device=options['device'])\n",
    "    else:\n",
    "        options['sampler'] = DDIMSampler(model, device=options['device'])\n",
    "    options['outpath'] = '../outputs/' + outpath\n",
    "    options['n_samples']= 1\n",
    "    options['batch_size'] = 1\n",
    "    options['n_rows'] = 1\n",
    "    options['sample_path'] = os.path.join(options['outpath'], \"samples\")\n",
    "    os.makedirs(options['sample_path'], exist_ok=True)\n",
    "    options['sample_count'] = 0\n",
    "    options['base_count'] = len(os.listdir(options['sample_path']))\n",
    "    options['grid_count'] = len(os.listdir(options['outpath'])) - 1\n",
    "    options['start_code'] = None\n",
    "    options['precision_scope'] = autocast\n",
    "    options['sampler'] = DDIMSampler(model, device=options['device'])\n",
    "    options['scale'] = 9\n",
    "    options['opt_C'] = 4\n",
    "    options['opt_H'] = h\n",
    "    options['opt_f'] = 8\n",
    "    options['opt_W'] = w\n",
    "    options['steps'] = steps\n",
    "    options['ddim_eta'] = 0.0\n",
    "    if rand_seed:\n",
    "        options['seed'] = np.random.randint(9999999)\n",
    "    else:\n",
    "        options['seed'] = 777\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(model, prompt, options):\n",
    "    seed_everything(options['seed'])\n",
    "    data = [options['batch_size'] * [prompt]]\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad(), \\\n",
    "        options['precision_scope'](\"cuda\"), \\\n",
    "        model.ema_scope():\n",
    "            conditionings = list()\n",
    "            unconditional_conditionings = list()\n",
    "            for n in trange(options['n_samples'], desc=\"Sampling\"):\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    uc = None\n",
    "                    if options['scale'] != 1.0:\n",
    "                        uc = model.get_learned_conditioning(options['batch_size'] * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "                    unconditional_conditionings.append(uc)\n",
    "                    conditionings.append(c)\n",
    "    end_time = time.time()\n",
    "    return unconditional_conditionings, conditionings, (end_time - start_time)\n",
    "\n",
    "\n",
    "def encode_samples(model, unconditional_conditionings, conditionings, options):\n",
    "    opt_C = options['opt_C']\n",
    "    opt_H = options['opt_H']\n",
    "    opt_f = options['opt_f']\n",
    "    opt_W = options['opt_W']\n",
    "    shape = [opt_C, opt_H // opt_f, opt_W // opt_f]\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad(), \\\n",
    "        options['precision_scope'](\"cuda\"), \\\n",
    "        model.ema_scope():\n",
    "            encoded_samples = list()\n",
    "            for uc, c in list(zip(unconditional_conditionings, conditionings)):\n",
    "                samples, _ = options['sampler'].sample(S=options['steps'],\n",
    "                                                    conditioning=c,\n",
    "                                                    batch_size=options['batch_size'],\n",
    "                                                    shape=shape,\n",
    "                                                    verbose=False,\n",
    "                                                    unconditional_guidance_scale=options['scale'],\n",
    "                                                    unconditional_conditioning=uc,\n",
    "                                                    eta=options['ddim_eta'],\n",
    "                                                    x_T=options['start_code'])\n",
    "                encoded_samples.append(samples)\n",
    "    end_time = time.time()\n",
    "\n",
    "    return encoded_samples, (end_time - start_time)\n",
    "\n",
    "\n",
    "def decode_imgs(model, encoded_samples, options):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad(), \\\n",
    "        options['precision_scope'](\"cuda\"), \\\n",
    "        model.ema_scope():\n",
    "            all_samples = list()\n",
    "            for samples in encoded_samples:\n",
    "                    x_samples = model.decode_first_stage(samples)\n",
    "                    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                    all_samples.append(x_samples)\n",
    "    end_time = time.time()\n",
    "\n",
    "    return all_samples, (end_time - start_time)\n",
    "\n",
    "\n",
    "def save_images(images, options, sample_path = 'samples'):\n",
    "    start_time = time.time()\n",
    "    sample_path = os.path.join(options['outpath'], sample_path)\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    for img in images:\n",
    "        base_count = len(os.listdir(sample_path))\n",
    "        img = Image.fromarray(img.astype(np.uint8))\n",
    "        img.save(os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "    end_time = time.time()\n",
    "\n",
    "    return (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../model_weights/v2-1_512-ema-pruned.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 220000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n"
     ]
    }
   ],
   "source": [
    "esrgan_model = initialize_esrgan()\n",
    "model = init_sd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = '''A realistic, highly-detailed australian shepherd catching a fish with \\\n",
    "    its mouth from a river in the countryside, mountainous background, beautiful \\\n",
    "    sunny day, symmetrical face, beautiful eyes, detailed eyes, detailed paws, \\\n",
    "    symmetrical legs, realistic fur, high-resolution.'''\n",
    "prompt2 = '''A suureal, stylistic australian shepherd catching an ethereal glowing fish with \\\n",
    "    its mouth from a majestic psychedelic river in the colorful countryside, mountainous background, beautiful \\\n",
    "    sunny day, symmetrical face, beautiful eyes, detailed eyes, detailed paws, \\\n",
    "    symmetrical legs, Salvador Dali, surreal.'''\n",
    "options = get_options_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 9417252\n",
      "data: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 72, 128), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:03<00:00, 16.53it/s]\n"
     ]
    }
   ],
   "source": [
    "ucs, cs, text_enc_time = encode_text(model, prompt1, options)\n",
    "encoded_samples, sample_enc_time = encode_samples(model, ucs, cs, options)\n",
    "raw_imgs, img_dec_time = decode_imgs(model, encoded_samples, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, upscale_time = upscale_images(raw_imgs, esrgan_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_time = save_images(images, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 9417252\n",
      "data: 100%|██████████| 1/1 [00:00<00:00, 38.72it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:00<00:00, 36.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 72, 128), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:02<00:00, 17.55it/s]\n",
      "Global seed set to 9417252\n",
      "data: 100%|██████████| 1/1 [00:00<00:00, 59.22it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:00<00:00, 55.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 72, 128), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:02<00:00, 18.25it/s]\n"
     ]
    }
   ],
   "source": [
    "ucs, cs, text_enc_time = encode_text(model, prompt1, options)\n",
    "encoded_samples1, sample_enc_time = encode_samples(model, ucs, cs, options)\n",
    "\n",
    "ucs, cs, text_enc_time = encode_text(model, prompt2, options)\n",
    "encoded_samples2, sample_enc_time = encode_samples(model, ucs, cs, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream_step_size = 100\n",
    "diff_vec = (encoded_samples2[0] - encoded_samples1[0]) / dream_step_size\n",
    "encoded_dream_samples = [encoded_samples1[0] + diff_vec * i for i in range(dream_step_size)]\n",
    "dream_samples, dream_gen_time = decode_imgs(model, encoded_dream_samples, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream_times = list()\n",
    "frame_times = list()\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "    encoded_dream_sample = encoded_samples1[0] + diff_vec * i\n",
    "    dream_sample, dream_gen_time = decode_imgs(model, encoded_dream_sample.unsqueeze(0), options)\n",
    "    dream = torch.cat(dream_sample, dim=0).to(torch.float32)\n",
    "    dream = dream.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "    dream = np.transpose(dream, (1, 2, 0))\n",
    "    dream = (dream * 255.0).round()\n",
    "    end_time = time.time()\n",
    "    dream_times.append(dream_gen_time)\n",
    "    frame_times.append(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20252366065979005"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(frame_times) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 1024, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dream.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "dreams = torch.cat(dream_samples, dim=0).to(torch.float32)\n",
    "dreams = dreams.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "dreams = np.transpose(dreams, (1, 2, 0))\n",
    "dreams = (dreams * 255.0).round()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_images(dreams, options, 'first_dream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 576, 1024, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dreams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow('Image', dreams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image in dreams:\n",
    "#     # image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#     cv2.imshow('Image', image)\n",
    "\n",
    "#     # Break the loop when 'q' is pressed\n",
    "#     if cv2.waitKey(41) == ord('q'):\n",
    "#         break\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dreams[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
